\documentclass{article}
\usepackage{times}
\usepackage{tikz}
\usepackage{verbatim}
\usepackage{hyperref} 
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{sign}
\DeclareMathOperator*{\Lik}{Lik}
\DeclareMathOperator*{\Peaks}{Peaks}
\DeclareMathOperator*{\HotSpots}{HotSpots}
\newcommand{\Cost}{\text{Cost}}
\usepackage{stfloats}
\DeclareMathOperator*{\Diag}{Diag}
\DeclareMathOperator*{\TPR}{TPR}
\DeclareMathOperator*{\Segments}{Segments}
\DeclareMathOperator*{\FPR}{FPR}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\maximize}{maximize}
\DeclareMathOperator*{\minimize}{minimize}
\newcommand{\ZZ}{\mathbb Z}
\newcommand{\NN}{\mathbb N}
\newcommand{\RR}{\mathbb R}

\usepackage{icml2015}

\icmltitlerunning{PeakSeg: constrained optimal segmentation and
  supervised penalty learning for peak detection in count data}

\begin{document}

\twocolumn[
\icmltitle{PeakSeg: constrained optimal segmentation and
  supervised penalty learning for peak detection in count data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Guillem Rigaill}{rigaill@evry.inra.fr}
\icmladdress{INRA, Evry, France}
\icmlauthor{Toby Dylan Hocking}{toby.hocking@mail.mcgill.ca}
\icmlauthor{Guillaume Bourque}{guil.bourque@mcgill.ca}
\icmladdress{McGill genome center, Montreal, Quebec, Canada}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{segmentation, count data, penalty functions}

\vskip 0.3in
]

\begin{abstract}
  Peak detection is a central problem in ChIP-seq data analysis, and
  current algorithms for this task are unsupervised and mostly
  effective for a single data type and pattern (e.g. H3K4me3 data with
  a sharp peak pattern). We propose PeakSeg, a new constrained maximum
  likelihood segmentation model for peak detection, and an efficient
  inference algorithm: constrained dynamic programming. We investigate
  un\-super\-vised and super\-vised learning of penalties for
  the critical model selection problem. We show that the 
  super\-vised method is the first algorithm with state-of-the-art peak
  detection across all data sets in a benchmark that includes both
  sharp (H3K4me3) and broad (H3K36me3) patterns.
\end{abstract}

\section{Introduction}

\subsection{Unsupervised ChIP-seq peak detection}

Chromatin immunoprecipitation sequencing (ChIP-seq) is a biological
experiment for genome-wide profiling of histone modifications and
transcription factor binding sites, with many experimental and
computational steps \citep{practical}. Briefly, each experiment yields
a set of sequence reads which are aligned to a reference genome, and
then the data are interpreted by counting the number of aligned reads
at each genomic position. In this paper we propose a new method for
peak calling these data, which is a binary classification problem for
each genomic position. The positive class is enriched (peaks) and the
negative class is background noise. Importantly, peaks and background
occur in long contiguous segments across the genome, as shown in
Figure~\ref{fig:dp-peaks-train}.

\begin{figure*}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-dp-peaks-train}
  \vskip -0.5cm
  \caption{Different ChIP-seq data types have peaks with different
    patterns. The goal of supervised peak detection is to learn the
    data set-specific peak pattern encoded in the annotated region
    labels (peakStart/peakEnd mean there should be exactly 1 peak
    start/end somewhere in the region, and noPeaks means there should
    be no overlapping peaks). Peaks detected by several algorithms are
    shown as line segments with an open circle, and for these data
    \ref{PeakSeg} with $p=2$ peaks achieves zero errors for both
    pattern types.}
  \label{fig:dp-peaks-train}
\end{figure*}

More concretely, a single ChIP-seq profile on a genomic region with
$d$ base pairs can be represented as a vector $\mathbf y= \left[
  \begin{array}{ccc}
    y_1 & \cdots & y_d
  \end{array}
\right]\in\ZZ_+^d$ of counts of aligned sequence reads. A peak
detection algorithm can be described as a function $c:\ZZ_+^d
\rightarrow \{0, 1\}^d$ which returns 0 for background noise and 1 for
a peak. In contrast to the
supervised method proposed in this paper, most previous algorithms are
unsupervised since they define a peak detector $c$ using only the
profile data $\mathbf y$.

\subsection{Supervised peak detection}

In supervised peak detection \citep{hocking2014visual}, there
are $n$ labeled samples, and each sample $i\in\{1, \dots, n\}$ has a
profile $\mathbf y_i\in\ZZ_+^d$ and a set of annotated region labels $R_i$
which defines a non-convex annotation error function
\begin{equation}
  \label{eq:error}
  E[c(\mathbf y_i),  R_i] =
  \text{FP}[c(\mathbf y_i), R_i] +
  \text{FN}[c(\mathbf y_i), R_i].
\end{equation}
The annotation error counts the number of false positive (FP) and
false negative (FN) regions, so it takes values in the non-negative
integers. The goal of learning is to find a peak caller~$c$ with
minimal error on some test profiles:
\begin{equation}
  \label{eq:min_error}
  \minimize_c \sum_{i\in\text{test}} E[c(\mathbf y_i),  R_i].
\end{equation}
More specifically, we suppose that the training data and the test data
exhibit the same pattern type.

\subsection{Different ChIP-seq peak patterns}

There are several different kinds of peak patterns that have been
observed and labeled in different ChIP-seq data sets. For example,
Figure~\ref{fig:dp-peaks-train} shows H3K4me3 data with a sharp peak
pattern, and H3K36me3 data with a broad peak pattern. Current peak
detectors are unsupervised learning algorithms that are designed for
specific patterns. For example, the MACS algorithm was designed for
the sharp pattern in H3K4me3 data \citep{MACS}.

Another example is HMCan \citep{HMCan}, whose authors suggest fixed
pattern-specific mergeDistance parameters: H3K4me3 $\Rightarrow 200$,
H3K36me3 $\Rightarrow 1000$. However, these parameters may not be
optimal for any given data set.

\subsection{Contributions and organization}

The main contribution of this paper is \ref{PeakSeg}, a peak detection
model that can be trained using supervised learning, and is effective
for several pattern types. Rather than taking such unsupervised model
parameters for granted, we propose to train model parameters using
labeled data of the same pattern type. Importantly, and in contrast to
existing unsupervised approaches which can only be trained using grid
search, we propose efficient discrete and convex optimization
algorithms for model training. Our method is the first peak detector
that uses supervised learning, and achieves accurate peak detection
across several data sets and patterns.

A second contribution of this paper is a detailed study of the peak
detection accuracy of several unsupervised and supervised segmentation
algorithms in a large benchmark ChIP-seq data set. The two main
results are that the oracle penalty of \citet{cleynen2013segmentation}
is more accurate than asymptotic penalties like the AIC/BIC, and that
supervised penalty learning can be used to further increase peak
detection accuracy.

The rest of this paper is organized as follows. In
Section~\ref{sec:related} we discuss related work and in
Section~\ref{sec:model} we propose the constrained \ref{PeakSeg}
model. In Section~\ref{sec:algorithms} we propose an efficient
algorithm, and in Section~\ref{sec:results} we show
state-of-the-art peak detection accuracy in a large benchmark database
of ChIP-seq profiles. We discuss the significance of these results in
Section~\ref{sec:discussion} and propose some directions for future
work in Section~\ref{sec:conclusions}.

\section{Related work}
\label{sec:related}

Previous work can be divided into roughly three categories:
unsupervised ChIP-seq peak detectors, maximum likelihood segmentation
models, and optimal penalty functions for segmentation.

\subsection{Unsupervised ChIP-seq peak detectors}

There are literally dozens of unsupervised algorithms for peak
detection in ChIP-seq data sets, and the bioinformatics literature
contains several published comparison studies \citep{evaluation2010,
  rye2010manually, chip-seq-bench}. 

However, supervised peak detection model training is a relatively new
idea proposed by \citet{hocking2014visual}. In that benchmark of
ChIP-seq data sets, there are two different histone mark types:
H3K4me3 (sharp peak pattern) and H3K36me3 (broad peak pattern). The
best peak detection algorithm for these H3K4me3 data was macs
\citep{MACS}, and the best for H3K36me3 was HMCan \citep{HMCan}. Both
of these algorithms are unsupervised, but were calibrated using the
annotated region labels to choose the best scalar significance
threshold hyperparameter via grid search. \citet{picking2012} also
attempted to train hyperparameters of unsupervised peak detectors, and
found that optimal values do not generalize between data sets. 

In contrast, we show in this paper that our supervised multi-parameter
\ref{PeakSeg} model can be trained for accurate peak detection on test
data that has the same type of peak pattern.

\subsection{Maximum likelihood segmentation models}

The \ref{PeakSeg} model we propose in this paper is a constrained
version of the Poisson segmentation model of \citet{Segmentor}.
Their unconstrained model can be computed using a dynamic programming
algorithm (DPA) \citep{bellman}, or a pruned dynamic programming
algorithm (pDPA) \citep{pruned-dp}. Both algorithms are guaranteed to
recover the exact solution to the unconstrained model, but there are
two important differences. The pDPA is more complicated to implement,
but is also computationally faster than the DPA. For segmenting a
sequence of $d$ data points, the pDPA takes on average $O(d\log d)$
time whereas the DPA takes $O(d^2)$ time. 

\begin{figure*}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-Segmentor-PeakSeg}
  %\input{figure-Segmentor-PeakSeg}
  \vskip -0.5cm
  \caption{Example profile $\mathbf y$ (grey), with green horizontal
    lines for the segmentation mean $\mathbf m$, and green vertical
    lines to emphasize change-points. For this particular profile
    $\mathbf y$, the unconstrained and contrained models are
    equivalent $\mathbf{\hat m}^s(\mathbf y) = \mathbf{\tilde
      m}^s(\mathbf y)$ for $s\in\{1, 3\}$ segments but not for
    $s\in\{5, 7\}$. For the constrained models, the even-numbered
    segments are interpreted as peaks (\ref{eq:peaks}), whose accuracy
    can be quantified using the annotated region labels (\ref{eq:error}).}
  \label{fig:Segmentor-PeakSeg}
\end{figure*}

In constrast to the unconstrained segmentation model of
\citet{Segmentor}, our proposed \ref{PeakSeg} model has an additional
constraint. Rather than searching all possible change-points to find
the optimal segmentation with $s$ segments, we propose to constrain
the possible change-points to the subset of models that can be
interpreted as peaks (Section~\ref{sec:constrained}).

Due to the simplicity of its implementation, we propose a constrained
dynamic programming algorithm (cDPA) that requires a small
modification to the standard DPA solver. Although the DPA is an exact
solver for the unconstrained problem, we show that the cDPA is a
heuristic that is not guaranteed to solve the constrained
problem (Section~\ref{sec:dp-fails}). However, we show that it is
still very useful and accurate in practice on real data
(Section~\ref{sec:results}).

\subsection{Penalty functions for segmentation}

For each segmentation problem, the proposed cDPA returns a sequence of
models with $s\in\{1, 3, \dots, s_{\text{\text{max}}}\}$ segments. The model
selection problem is choosing a sample-specific number of segments $s$
that maximizes the number of true positive peak detections while
minimizing the number of false positives. For example, in the sample
of Figure~\ref{fig:Segmentor-PeakSeg}, an ideal model selection
procedure would choose $s\in\{3, 5\}$ segments, since those models
have zero errors (\ref{eq:error}).

There are many unsupervised penalty functions for model selection. For
example, the classical AIC \citep{Akaike73} and BIC \citep{Schwarz78} are based
on theoretical asymptotic arguments. Several attempts have been made
to adapt the BIC for segmentation problems \citep{Yao88,
  mBIC}, and the non-asymptotic model selection theory of
\citet{cleynen2013segmentation} suggests another penalty function.

In the supervised setting of this paper, there is a training data set
of segmentation problems $i\in\{1, \dots, n\}$, with labeled regions
$R_i$ that can be used to compute the annotation error
(\ref{eq:error}). Thus, rather than taking a particular unsupervised
penalty function for granted, we can use the training data to learn
the penalty function with minimum error. In particular in
Section~\ref{sec:supervised-multi} we propose using the max margin interval
regression algorithm of \citet{HOCKING-penalties}, which was
originally proposed for learning a penalty function for optimal
change-point detection.

\section{From unconstrained to constrained maximum likelihoood
  segmentation}
\label{sec:model}

In this section we discuss Poisson maximum likelihood segmentation
models for count data. First, we discuss an existing unconstrained
model, and then we propose a constraint that makes the model suitable
for peak detection.

\subsection{Unconstrained maximum likelihood segmentation}

For a sequence of $d$ data points to segment, we fix a maximum number
of segments $1 \leq s_{\text{\text{max}}}\leq d$, and define the
unconstrained maximum likelihood segmentation problem for
any $s\in\{1, 2, \dots, s_{\max}\}$ as
\begin{align*}
  \label{unconstrained}
  \mathbf{\hat m}^s(\mathbf y)  =\ 
  &\argmin_{\mathbf m\in\RR^{d}} && 
  \rho
  \tag{\textbf{Unconstrained}}
  (\mathbf m, \mathbf y) \\
  &\text{such that} && \Segments(\mathbf m)=s,
\end{align*}
where the Poisson loss function is
\begin{equation}
  \rho(\mathbf m, \mathbf y)= \sum_{j=1}^d m_j - y_j \log m_j.
\end{equation} 
The model complexity is the number of piecewise constant segments
\begin{equation}
  \Segments(\mathbf m)=1+\sum_{j=2}^d I(m_j \neq m_{j-1}),
\end{equation}
where $I$ is the indicator function. 

Although it is a non-convex optimization problem, the sequence of
segmentations $\mathbf{\hat m}^1(\mathbf y), \dots, \mathbf{\hat
  m}^{s_{\text{\text{max}}}}(\mathbf y)$ can be computed in
$O(s_{\text{\text{max}}} d^2)$ time using dynamic programming
\citep{bellman}, or in $O(s_{\text{\text{max}}} d \log d)$
time using pruned dynamic programming \citep{pruned-dp, Segmentor}.

This problem is ``\ref{unconstrained}'' in the sense that
$\mathbf{\hat m}^s(\mathbf y)$ is the most likely segmentation of all
possible models with $s$ piecewise constant segments ($s-1$
change-points). Several unconstrained models are shown on the left of
Figure~\ref{fig:Segmentor-PeakSeg}, and for example the second segment of the
model with $s=3$ segments appears to capture the peak in the data.
% In general, we would like to use the 2nd, 4th,
% ... segments as peaks, and the 1st, 3rd, ... segments as
% background. 
To construct a peak detector $c$, we first define the peak indicator at base
$j\in\{2, \dots, d\}$ as
\begin{equation}
  \label{eq:peaks}
  P_j(\mathbf m) = \sum_{k=2}^j \sign( m_{k} - m_{k-1} ),
\end{equation}
where $P_1(\mathbf m)=0$ by convention. $P_j(\mathbf m)$ is the
cumulative sum of signs of changes up to point $j$ in the piecewise constant
vector $\mathbf m$. 

Then we define a peak detector $c(\mathbf y) = \mathbf P\left[
  \mathbf{\tilde m}^s(\mathbf y) \right]$, where
\begin{equation}
  \mathbf
P[\mathbf m] = \left[\begin{array}{ccc} P_1(\mathbf m) & \cdots &
    P_d(\mathbf m)
\end{array}\right].
\end{equation}

\subsection{PeakSeg: constrained maximum likelihood}
\label{sec:constrained}

In general for the unconstrained model $P_j(\mathbf m)\in\ZZ$, which
is problematic since we want to use it as a peak detector with binary
outputs $P_j(\mathbf m)\in \{0, 1\}$. 

For example, if $\mathbf m = \left[\begin{array}{ccccccc}1.1 &
    1.1 & 2 & 2 & 4 & 4 & 3\end{array}\right]$, with two changes up
followed by one change down, then $P(\mathbf m) =
\left[\begin{array}{ccccccc}0 & 0 & 1 & 1 & 2 & 2 &
    1 \end{array}\right]$.

This is also a practical problem that can be observed in real data
sets. For example in Figure~\ref{fig:Segmentor-PeakSeg} there is a position $j$ for
which $P_j\left[ \mathbf{\hat m}^5(\mathbf y) \right]=2$ (since the
mean changes up, up, down, down). 

Thus we constrain the peak indicator $P_j(\mathbf
m)\in\{0, 1\}$, which results
in the constrained problem
\begin{align*}
  \label{PeakSeg}
  \mathbf{\tilde m}^s(\mathbf y)  =
    \argmin_{\mathbf m\in\RR^{d}} &\ \ 
    \rho(\mathbf m, \mathbf y) 
    \tag{\textbf{PeakSeg}}
\\
    \text{such that} &\ \  \Segments(\mathbf m)=s,  \\
     \forall j\in\{1, \dots, d\}, &\ \ P_j(\mathbf m) \in\{0, 1\}.
\end{align*}
Note that one must specify the number of segments $s$ or,
equivalently, the number of peaks $p=(s-1)/2$. Another way to
interpret the constrained \ref{PeakSeg} problem is that the sequence
of changes in the segment means $\mathbf m$ must begin with a positive
change and then alternate: up, down, up, down, ... (and not up, up,
down). Thus the even-numbered segments may be interpreted as peaks,
and the odd-numbered segments may be interpreted as background.

For example, Figure~\ref{fig:dp-peaks-train} shows the $p=2$ peaks
detected on the second and fourth segments of the \ref{PeakSeg}
solution for $s=5$ segments.

Figure~\ref{fig:Segmentor-PeakSeg} shows a profile where the constraint is
necessary for the even-numbered segments to be interpreted as
peaks. In particular, it is clear that unconstrained models with
$s\in\{5, 7\}$ segments do not satisfy $P_j[\mathbf{\hat m}^s(\mathbf
y)]\in\{0, 1\}$ for all positions $j\in\{1,\dots, d\}$ (since they
have up, up, down changes).

\section{Algorithms}
\label{sec:algorithms}

In this section we propose efficient algorithms for computing the
\ref{PeakSeg} model, and for choosing the number of segments $s$. We
first review the existing standard dynamic programming algorithm (DPA)
for segmentation, then propose a new constrained dynamic programming
algorithm (cDPA) that can be used to recover a sequence of
\ref{PeakSeg} models, $\mathbf{\tilde m}^1(\mathbf y), \dots,
\mathbf{\tilde m}^{s_{\text{\text{max}}}}(\mathbf y)$
(Section~\ref{sec:constrained-dp}). Then, we show how the max margin
interval regression algorithm of \citet{HOCKING-penalties} can be used
to learn a penalty function for choosing the number of segments
$s\in\{1, \dots, s_{\text{\text{max}}}\}$
(Section~\ref{sec:supervised-multi}).

\subsection{The standard DPA}

As already said, the best segmentation of the Poisson model can be
recovered using dynamic programming. To describe this algorithm we
define $\hat{\mathcal{L}}_{s,t}$ as minus the best log-likelihood of
the segmentation $\hat{m}_{s,t}$.  This best log-likelihood can be
written as the sum of the log-likelihood of its segment.  For the
Poisson model the log-likelihood of a segment from $t'$ to $t$ is
\begin{equation}
  \label{eq:log-lik-segment}
  c_{t',t} = (\sum_{j=t'+1}^t y_j) - (\sum_{j=t'+1}^t y_j)
\log(\frac{\sum_{j=t'+1}^t y_j}{t-t'}).
\end{equation}
The key idea behind the DP algorithm is that any subsegmentation
of an optimal segmentation should be optimal. 
This can be formally written as the following update rule :
\begin{equation}
\widehat{\mathcal{L}}_{s,t}= \min_{t' \leq t} \{ \ \widehat{\mathcal{L}}_{s-1,t'} + c_{t',t} \}. \label{eq:update1}
\end{equation}
Using this update rule iteratively for all $s$ from $2$ to
$s_{\text{max}}$ and for all $t$ from $2$ to $d$ we recover the best
segmentations in $1$ to $s_{\text{max}}$ in
$\mathcal{O}(s_{\text{max}}d^2)$ time.  Note that we do not need to
store in memory the $d(d-1)/2$ matrix of $c_{t',t}$.  That is because
$c_{t',t}$ is a simple function $\sum_{j=t'+1}^t y_j$ which can be
computed in $\mathcal{O}(1)$ given the cumulative sum of the
data-points, $\sum_{j=1}^t y_j$.

%\begin{algorithm}[H]
%\begin{algorithmic}[1]
%\FOR{$d=2$ to $D_{\text{max}}$}
%\FOR{$\tau'=d$ to $n$}
%\STATE $ \mathbf{L}_{d,\tau'+1} = \min_{\tau \leq \tau'} \{ \ \mathbf{L}_{d-1,\tau} + C_{\tau, \tau'+1} \ \}$
%\STATE $ \mathbf{m}_{d,\tau'+1} = \arg \min_{\tau \leq \tau'} \{ \ \mathbf{L}_{d-1,\tau} + C_{\tau, \tau'+1} \ \}$
%\ENDFOR
%\ENDFOR
%\caption{Dynamic Programming for Poisson segmentation model}
%\end{algorithmic}\label{algo:v1}
%\end{algorithm}

\subsection{cDPA: a simple modification}
 \label{sec:constrained-dp}

 In order to enforce the segmentation to obey the constraint of the
 \ref{PeakSeg} model, we introduce a simple modification of the
 previous update rule (\ref{eq:update1}). %algorithm %\ref{algo:v1}.
 Up to now we considered all possible $t'$ smaller than the current
 $t$. Here we propose to consider only those such that the resulting
 segmentation can be interpreted as a succession of peaks. To make
 this idea more precise we introduce the notation $\mathbf{E}_{s,t}$
 which is the last empirical mean of the best segmentation in $s$ up
 to $t$.  We also define $M_{t', t} = (\sum_{j=t'+1}^t y_j)/(t-t')$ as
 the empirical mean of the segment $[t', t]$.

Then if $s$ is even (peak segment) the mean of segment $[t', t]$
should be larger than the previous one and in the update rule we only
consider $t'$ such that
$$ t' \leq t \ | \ \mathbf{E}_{s-1,t'} \leq M_{t', t}.$$
On the contrary, if $s$ is odd (background segment) the mean of
segment $[t', t]$ should be smaller than the previous one and we
consider only $t'$ such that
$$ t' \leq t \ | \ \mathbf{E}_{s-1,t'} \geq M_{t', t}.$$
In the end we get the following procedure:
\begin{algorithm}[H]
\begin{algorithmic}[1]
\FOR{$s=2$ to $s_{\text{max}}$}
\FOR{$t=s$ to $d$}
\IF{$d$ is even}
\STATE $ \widehat{\mathcal{L}}_{s,t} = \underset{t' \leq t| \mathbf{E}_{s-1,t} \leq M_{t', t}} {\min}\{ \ \widehat{\mathcal{L}}_{s-1,t'} + c_{t', t} \ \}$
\ENDIF
\IF{$d$ is odd}
\STATE $ \widehat{\mathcal{L}}_{s,t} = \underset{t' \geq t | \mathbf{E}_{s-1,t'} \leq M_{t', t} }{\min} \{ \ \widehat{\mathcal{L}}_{s,t'} + c_{t',t} \ \}$
\ENDIF
\ENDFOR
\ENDFOR
\caption{Constrained dynamic programming (cDPA) for Poisson
  segmentation model}
\end{algorithmic}\label{algo:v2}
\end{algorithm}

For simplicity we omitted the update of $\mathbf{E}_{s,t}$ in the
previous procedure, it can be found by recovering the mean of the best
last segment.

Our proposed cDPA explores the set of segmentations and return one
which is compatible with the constraint of the \ref{PeakSeg}
model. Importantly, this procedure is not guaranteed to recover a
segmentation. For example for an ever increasing series of data $y_1 <
\cdots < y_d$ there is no model that satisfies the \ref{PeakSeg} model
constraints, and the cDPA thus returns no model (TODO: write that
explicitly?).

\subsection{The cDPA is a heuristic}
\label{sec:dp-fails}

Note also that the cDPA is a heuristic in the sense that it is not
clear which optimization problem it solves.  In particular, it does
not necessarily recover the best possible (in terms of likelihood)
segmentation that satistifies the \ref{PeakSeg} model constraints.  We
illustrate this with the following example with
$\mathbf y = \left[\begin{array}{cccc} 1 & 10 & 13 & 11
\end{array}\right]\in\ZZ_+^4
$.  There are only 3 possible segmentations: $[1][10][14, 11]$,
$[1][10, 14][13]$ and $[1, 10][14][13]$.  If we use max-likelihood
estimates for each segment mean, then only the last segmentation obeys
the \ref{PeakSeg} model constraints. However the cDPA will not recover
it.  That is because the best segmentation in $2$ segments up to point
$3$ (recovered by the cDPA for $s=2$) is $[1][10, 14]$. Its likelihood
is $(1 + 0) + (10+14 + (10+14)\log(24/2)) \approx 84.6 $ which is
indeed larger than the likelihood of $[1, 10][14]$: $(11 +
11\log(11/2)) + (14\log(14)) \approx 66.69 $. For $s=3$ the cDPA will
then consider the segmentations $[1][10] + [14, 13]$ and $[1][10, 14]
+ [13]$ and discard them since they do not satisfy the \ref{PeakSeg}
model constraints. The cDPA will thus report that it didn't find a
valid peak model in $s=3$. However there is one: $[1, 10][14][13]$.

Note that if we run the constrained DP in the backward direction, 
$\mathbf y = \left[\begin{array}{cccc} 13 & 14 & 10 & 1
\end{array}\right]\in\ZZ_+^4
$, we would recover the best solution $[13][14][10, 1]$.


% \subsection{Constrained dynamic programming}
% \label{sec:constrained-dp}

% We propose to use dynamic programming to compute the sequence of
% maximum likelihood models $\mathbf{\tilde m}^1(\mathbf y), \dots,
% \mathbf{\tilde m}^{s_{\text{\text{max}}}}(\mathbf y)$ satisfying this up-down
% constraint.

% \textbf{TODO}: more detailed description of dynamic programming.

% The algorithm is in $O(s_{\text{\text{max}}} d^2)$ time, where $d$ is the
% number of data points, using the compression scheme proposed by
% \citet{Segmentor}. A free/open-source R package implementing the
% dynamic
% programming is available at\\
% \url{https://github.com/tdhock/PeakSegDP}

% \subsection{Constrained DP is a heuristic}
% \label{sec:dp-fails}
% \textbf{TODO}: discussion of when the DP does not recover the optimal
% solution. Maybe a figure? Maybe not. Maybe a discussion of how many
% times it returned all 10 models in the real data sets.



\subsection{Unsupervised penalty learning}
\label{sec:unsupervised}

% After computing the constrained maximum likelihood segmentations for
% each sample $i\in\{1,\dots, n\}$, the only question that remains is:
% how many segments?

For a profile $\mathbf y\in\RR^d$ and a penalty $\lambda\in\RR_+$, the
optimal number of segments is
\begin{equation}
  \label{eq:optimal_segments}
  s^*(\lambda, \mathbf y) =
  \argmin_{s\in\{1,3,\dots, s_{\text{\text{max}}}\}}
  \rho\left[
    \mathbf{\tilde m}^s(\mathbf y),
    \mathbf y
  \right]
  + h(s, d) \lambda,
\end{equation}
where $h(s, d_i)$ is a model complexity function that is given. In
this paper we consider two types of model complexity $h$ functions:
AIC/BIC and oracle (Table~\ref{tab:penalties}). 

There are several unsupervised penalties that are based on theoretical
assumptions about the data (distribution, independence) which may or
may not be true in practice. One can use asymptotic arguments to get a
penalty such as the AIC \citep{Akaike73}, BIC \citep{Schwarz78}, or
mBIC \citep{mBIC}. For example, the AIC corresponds to a penalty of
$\lambda=2$ and a model complexity of $h(s, d)=s$. More recently,
\citet{cleynen2013segmentation} applyied finite sample model selection
theory to obtain a more complicated model complexity $h$
(Table~\ref{tab:penalties}) and a heuristic for computing the penalty
$\lambda$. All of these penalty functions are completely unsupervised
since they ignore the annotated region labels in the training data
set.

These models are named *.0 in Table~\ref{tab:penalties} and
Figure~\ref{fig:test-error}.

\subsection{Supervised single-parameter penalty learning}
\label{sec:supervised-single}

\citet{cleynen2013segmentation} proposed to use the un\-supervised
heuristic of \citet{Lav05} for calibrating the penalty constant
$\beta$ in equation (6) of their paper. Instead, we propose to use the
annotated region labels as a super\-vised method for calibrating the
penalty constant $\beta$. Note that their $\beta$ is the same as our
$\lambda$ in (\ref{eq:optimal_segments}). 

More specifically, we defined a grid of 200 penalty values evenly
spaced on the log scale between $10^{-2}$ and $10^4$, then used grid
search to select the value that minimizes the annotation error
(\ref{eq:error}) on the train set.

These models are named *.1 in Table~\ref{tab:penalties} and
Figure~\ref{fig:test-error}.

\subsection{Supervised multi-parameter penalty learning}
\label{sec:supervised-multi}

For supervised learning of multi-parameter penalty functions, we can
use the max margin interval regression algorithm of
\citet{HOCKING-penalties}.
We define sample-specific features $\mathbf x_i\in\RR^m$ and penalty
values $\log \lambda_i = f(\mathbf x_i) = \beta + \mathbf w^\intercal
\mathbf x_i$, which is an affine function with parameters
$\beta\in\RR,\mathbf w\in\RR^m$ that will be learned. 
For each sample
$i\in\{1,\dots, n\}$ we select $s^*[\exp f(\mathbf x_i), \mathbf y_i]$
segments.

% The learning algorithm amounts to minimizing a
% smooth convex loss $\ell_i:\RR\rightarrow\RR_+$ which depends on the
% annotated region data $R_i$,
% \begin{equation}
%   \label{eq:relax}
%   \mathbf{\hat w}^\gamma, \hat \beta^\gamma = \argmin_{\mathbf w\in\RR^m, \beta\in\RR}
%   \gamma ||\mathbf w||_1 +
%   \sum_{i=1}^n
%   \ell_i\left[  \beta+ \mathbf w^\intercal \mathbf x_i \right],
% \end{equation}
% where $\gamma\in\RR_+$ is the degree of regularization. Since
% $\ell_i$ is smooth and convex, this problem can be easily solved using
% proximal gradient algorithms such as FISTA \citep{fista}.

As shown in Table~\ref{tab:penalties}, we considered learning models
with 3 and 41 parameters:

\begin{itemize}
\item \textbf{3}: we used an $m=2$-dimensional feature
  vector $\mathbf x_i = \left[\begin{array}{cc} \log\max \mathbf y_i &
      \log d_i
\end{array}\right]$ where $d_i$ is the number of base pairs for 
sample/chromosome $i$.  This corresponds to a penalty $\lambda_i =
e^\beta (\max\mathbf y_i)^{w_1} d_i^{w_2}$. We fit the model by
solving the un-regularized interval regression problem of
\citet{HOCKING-penalties}.
\item \textbf{41}: we defined $m=40$ features using transforms ($x,
  \log x, \log[x+1], \log\log x$), where $x$ are features such as
  quartiles of $\mathbf y_i$, mean of $\mathbf y_i$, and
  number of data points $d_i$. We fit the model by solving
  the L1-regularized problem of
  \citet{HOCKING-penalties}.
\end{itemize}

After learning a penalty function $\hat f$ on the training data, we
use the following method to make a prediction on a test sample with
profile $\mathbf y$ and features $\mathbf x$. We compute the predicted
penalty $\hat \lambda = \exp \hat f(\mathbf x)$, the predicted number
of segments $\hat s = s^*(\hat \lambda, \mathbf y)$, and finally the
predicted peaks $\mathbf P\left[ \mathbf{\tilde m}^{\hat s}(\mathbf y)
\right]$.

\begin{table*}[b!]

  \begin{minipage}{0.4\textwidth}
  \begin{tabular}{ll}
    \textbf{name} & \textbf{model complexity} $h(s, d_i)$ \\
    \hline
    AIC/BIC.* & $s$\\
    oracle.* & $s\left(1 + 4\sqrt{1.1 + \log(d_i/s)}\right)^2$
  \end{tabular}
  \end{minipage}
  \begin{minipage}{0.5\textwidth}
  \begin{tabular}{lllll}
    \textbf{name} & \textbf{smoothing} $\lambda_i$ & 
    \textbf{parameters} & \textbf{learning algorithm} \\
    \hline
    *.0 & AIC=2, BIC=$\log d_i$ & none & unsupervised \\
    *.1 & 
    $\beta$ & 
    $\beta\in\RR_+$ & grid search \\
    *.3 & 
    $e^\beta d_i^{w_1} (\max \mathbf y_i)^{w_{2}}$ & 
    $\beta, w_1, w_{2}\in\RR$ & interval regression \\
    *.41 & 
    $\exp(\beta + \mathbf w^\intercal \mathbf x_i)$ & 
    $\beta\in\RR, \mathbf w\in\RR^{40}$ & 
    regularized int. reg. \\
  \end{tabular}
  \end{minipage}

  \caption{Penalty functions considered in this paper are of the form
    $h(s, d_i) \lambda_i $, and are named according to model complexity
    $h(s, d_i)$ (left) and number of parameters to learn in $\lambda_i$ (right).
    For example, the AIC/BIC.41 model has a penalty of
    $s \exp(\beta + \mathbf w^\intercal \mathbf x_i)$, 
    where the parameters $\beta,\mathbf w$ 
    are learned using regularized interval regression 
    on a training data set.
  }
  \label{tab:penalties}
\end{table*}

\begin{figure*}[b!]
  \centering
  \includegraphics[width=\textwidth]{figure-dp-peaks-regression-dots}
  \vskip -0.5cm
  \caption{Test error of peak detection algorithms on seven labeled
    data sets (each point shows one of six randomly selected
    train/test splits, the shaded circle is the mean test error, and
    the vertical black line is the mean of the best algorithm for each
    data set). Data set names show histone mark type (e.g. H3K36me3),
    annotator (AM), and cell types (immune). Colors show how the
    training data were used to learn model parameters.}
  \label{fig:test-error}
\end{figure*}

\section{Results: state-of-the-art peak detection
  for two data types}
\label{sec:results}

We downloaded 7 benchmark data sets, which included a total of 12,826
manually annotated regions across 2,752 separate segmentation
problems\footnote{\url{http://cbio.ensmp.fr/~thocking/chip-seq-chunk-db/}}.
Each data set contains labels grouped into windows of nearby regions
(from 4 to 30 windows per data set). For each data set, we randomly
divided the annotated windows into half train and half test.  As shown
in Table~\ref{tab:penalties}, we considered 3 kinds of model training:
\begin{itemize}
\item \textbf{Unsupervised} uses the default parameters of each
  algorithm, ignoring the labeled data in the train set.
\item \textbf{Grid search} learns a scalar parameter with minimal
  annotation error on the train set.
\item \textbf{Interval regression} is the efficient multi-parameter
  penalty learning model of \citet{HOCKING-penalties}.
\end{itemize}

\subsection{Effectiveness of heuristic cDPA}

To run the cDPA on the benchmark data set, we first set the maximum
number of segments $s_{\text{\text{max}}}=19$, meaning a maximum of $p=9$
peaks. Running the cDPA on all of the data sets in the benchmark took
a total of about 6.5 days. For the largest profile we considered
($d=263,169$ weighted data points after compression), the cDPA took
about 155 minutes. The source code for benchmarking 
is at\\
\url{https://github.com/tdhock/PeakSeg-paper}

As discussed in Section~\ref{sec:dp-fails}, the cDPA is a heuristic
for computing the \ref{PeakSeg} model. To assess whether or not this
is a problem in real data sets, we checked for how many segmentation
problems the cDPA returned the complete sequence of 10 models
$\mathbf{\tilde m}^1(\mathbf y),\mathbf{\tilde m}^3(\mathbf y), \dots,
\mathbf{\tilde m}^{19}(\mathbf y)$. For the vast majority of
segmentation problems (2738/2752=99.5\%), the constrained DP returned
all 10 models. For the other 14 problems, the algorithm did not return
at least one of the ten models. Of these 14 problems with missing
models, 11 problems still had at least one perfect peak detection
model with zero annotation error~(\ref{eq:error}). We concluded that
although the cDPA is technically a heuristic algorithm for
solving \ref{PeakSeg}, it is still a useful peak detector in the vast
majority of real data sets.

% \subsection{Baseline peak detection algorithms}

% We compared the proposed PeakSeg algorithm to the two previous
% state-of-the-art peak detectors on this benchmark, macs and
% \mbox{hmcan.broad}. 

\subsection{Comparison of unsupervised methods}

In a previous study of this benchmark data set
\citep{hocking2014visual}, the macs algorithm was found to be the most
accurate peak detector for H3K4me3 data, whereas hmcan.broad was best
for H3K36me3 data.

Figure~\ref{fig:test-error} shows that in three H3K36me3 data sets
(broad peak pattern), hmcan.broad.0 is clearly more accurate than
macs.0. Furthermore, it is clear that the oracle.0 model is at least
as accurate as hmcan.broad.0.

For the four H3K4me3 data sets (sharp peak pattern), oracle.0 is about
as accurate as macs.0, which is clearly more accurate than
hmcan.broad.0.

Finally, the unsupervised AIC/BIC.0 method is clearly the least
accurate of all methods, since it always picks the model with the most
peaks (same as no penalty). We also tried the modified BIC of
\citet{mBIC}, but obtained the same high false positive rates.

\subsection{Supervised learning of 1 parameter increases peak
  detection accuracy}

\textbf{TODO}: discussion.

\subsection{Supervised learning of several parameters further 
  increases peak detection accuracy}

\textbf{TODO}: discussion of unsupervised vs 1-parameter vs multi-parameter
learning.

% \begin{center}
%   \input{table-dp-peaks-regression}
% \end{center}

\section{Discussion}
\label{sec:discussion}

\subsection{Supervised versus unsupervised learning}

\textbf{TODO}: discuss machine learning for recognizing different
patterns, versus using several different unsupervised algorithms for
specific data sets. E.g. car/face detector versus generic object
detector trained on various different data sets.

\subsection{Oracle penalty versus AIC/BIC penalty}

\textbf{TODO}

% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=\textwidth]{figure-regularized-all}
%   \vskip -0.5cm
%   \caption{Normalized weights of the 2-feature log.bases.log.max model
%     compared to the 41-feature L1.reg model. In some data sets the
%     L1.reg model sets these feature weights to 0.}
%   \label{fig:regularized-all}
% \end{figure*}

% \begin{figure*}[t!]
%   \centering
%   \includegraphics[width=\textwidth]{figure-regularized-feature-importance}
%   \vskip -0.5cm
%   \caption{Importance of features in L1.reg models. There were several
%     features in H3K4me3 data sets which were selected in all 6
%     train/test splits (note that unweighted.quartile.100\% is the same
%     as the maximum of the data). For other data sets it is not clear
%     which features are most important.}
%   \label{fig:regularized-feature-importance}
% \end{figure*}

\section{Conclusions and future work}
\label{sec:conclusions}

We proposed to use the solution of a constrained optimal segmentation
problem \ref{PeakSeg} as a ChIP-seq peak detector. For
$d$ data points and $s_{\text{\text{max}}}$ segments, we proposed to use
dynamic programming to compute the solutions in $O(s_{\text{\text{max}}} d^2)$
time. Furthermore, we proposed to use annotated region data as
supervision in a penalty learning problem (\ref{eq:relax}). This
approach yields state-of-the-art test error rates for peak detection
in a benchmark that includes both H3K4me3 and H3K36me3 data
sets.

Although it has been previously proposed to train a peak detector
using a set of positive control peak regions \citep{DFilter}, PeakSeg
is the first algorithm that can be trained with both positive and
negative control regions. PeakSeg is also the first algorithm to
exhibit state-of-the-art accuracy on both sharp H3K4me3 and broad
H3K36me3 ChIP-seq profiles in the benchmark of
\citet{hocking2014visual}. In the future, even better accuracy may be
obtained by engineering better features $\mathbf x_i$ for the penalty
learning problem (\ref{eq:relax}), perhaps based on Poisson
segmentation model selection theory \citep{cleynen2013segmentation}.

The current implementation of PeakSeg using dynamic programming has
one major limitation. The $O(s_{\text{\text{max}}} d^2)$ time complexity has
limited its application to subsets of chromosomes of up to $d=263,169$
data points. In comparison, the largest chromosome in hg19 (chr1) has
$d=249,250,621$ base pairs. To apply PeakSeg to larger data sets, we
are investigating a constrained version of pruned dynamic programming
\citep{pruned-dp, Segmentor}, which has $O(s_{\text{\text{max}}} d\log d)$
time complexity.

Finally, we are interested in segmenting multiple samples $i$ at the
same time, since peaks are often observed in the same genomic location
across several samples of the same cell type. This joint peak
detection problem may lead to more accurate peak calls, but it is a
considerably more difficult segmentation problem.

% \begin{table*}[b!]
%   \centering
%   \begin{tabular}{ccc}
%     \textbf{learning algorithm} & \textbf{heuristics} & 
%     \textbf{constrained optimization} \\
%     \hline
%     unsupervised & hmcan.broad.0, macs.0 & AIC/BIC.0, oracle.0 \\
%     grid search & hmcan.broad.1, macs.1 & AIC/BIC.1, oracle.1 \\
%     interval regression & --- & AIC/BIC.3, oracle.3 \\
%     regularized interval regression & --- & AIC/BIC.41, oracle.41 \\
%   \end{tabular}
%   \caption{Comparison of peak detection methods for ChIP-seq data. 
%     The two main contributions of this paper 
%     are to show that peak detection can be improved 
%     using constrained optimization and 
%     interval regression to learn a 
%     multi-parameter penalty function.}
%   \label{tab:supervised-maxlik}
% \end{table*}

\newpage


\bibliographystyle{abbrvnat}

\bibliography{refs}

\end{document}
